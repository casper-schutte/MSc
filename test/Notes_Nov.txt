
01/11/21:
    - Have 2 weeks to finish all the coding so that I have 6 weeks to write up.

    - To do:
        1) Fix the code, it cannot handle the positions resetting for different chromosomes.

        2) Test the algorithm/tool on real data

        Sounds easy, but I have not been able to do either.

    - I can't start testing on real data because the algorithm might change. So the priority is fixing the code.
        The problem is that when the consecutive blocks are calculated, they are not associated with the read
        name or number. In the previous iteration of the script, position (relative to the reference) was unique.
        Now, with the introduction of multiple chromosomes, the positions are no longer unique. Thus, I need to change
        the way the consecutive blocks are located or find a way to keep additional data such that the blocks are still
        identifiable.

    - One way I can overcome the problem is by using a different algorithm to return nonconsecutive blocks. I like the
        algorithm. The other way is to try to keep track of the read name while using the same algorithm. I will try
        the latter first.

02/11/21:
    - Made a lot of progress today. The script "consecutive_block_test.py" is working well using the new method.
        There are one or two kinks that might have to be worked out, but I will integrate it into my code and test
        different rearrangements. Then I will test using real data.
    - One thing I need to do is figure out how to extract the "AS" field as this will likely help me to assign borders
        more than just using the mapping quality (MAPQ). Since for FastA files there are reads that were clipped
        but still had a perfect mapping score (uniqueness).

03/11/21:
    - Will conduct some tests today to verify how the adapted algorithm works with different kinds of rearrangements.
        The results will be written up in "experiment_5.txt."
    - First round of tests were successful. Will continue tomorrow.

04/11/21:
    - Will continue with the tests of the new script. As soon as that is done I can start testing real data.

05/11/21:
    - Continued with testing the script on artificial rearrangements. See "experiment_5.txt"

06/11/21:
    - Should be finishing experiments on artificial data. Might make changes on what and how things are returned.

07/11/21:
    - Needed to make small changes. Results very good so far (see experiment_5.txt)
    - Testing multichromosoma data, see experiment_6.txt

08/11/21:
    - Finished experiment 6, see experiment_6.txt for results.
    - Ecerything is working as it should.

09/11/21:
    - Need to incorporate the new algorithm with the full script, so that we can also look at reads that did
        not map properly. These reads should correlate with the borders of rearrangements.

10/11/21:
    - Trying to fix the way that the algorithm deals with duplications. One of the duplicated pair of reads "maps" to
        the right place but the other either does not or does map but in both cases the MAPQ is 1. I would like to
        combine these reads with terrible quality scores into a single block (if they are continuous). This would
        make the output much more clear.
    - Decided it would be better if the borders and blocks were written to a .txt file instead of only being printed
        in the terminal.

13/11/21:
    - Will be trying to change the way reads are generated
    - Changing the way Bowtie2 returns reads such that coverage is increased. Multiple reads will be constructed from
        the same start position but of different lengths.
    - Did not find a way to do this in Bowtie2, but Samtools can do it:

    $ Samtools view -q (MAPQ below which reads are not returned) file_name.sam  > file_name.filtered.sam

    - After filtering out the reads with very low MAPQ (1 or 0, which were the ones that did not map or were
        duplicated), the real data proved too "noisy." My algorithm was too sensitive. Mapping single, slightly
        overlapping reads to construct continuous blocks worked very well for the artificial data. However, the real
        data proved much too noisy. Small differences in the genomes (and in the way reads happen to be mapped -
        remember that Bowtie2 uses random seeds to start looking for alignments).

    - I need to change the way reads are constructed. Multiple reads (of varying lengths) will be constructed from the
        same starting position. This way, the reads will all map where the genomes are the same. Where there is a
        breakpoint, the shorter reads should map but the longer ones map less and less well. This way, small
        differences will not be falsely returned as breakpoints. Instead, the longer (and further along) reads will
        start mapping again, confirming the small change or single badly mapped read is not a breakpoint.

    - We can make the reads overlap more, keeping the current method of construction and return, but I think that
        a more accurate method would use reads of a variable length.

    - Kept the read length at 150 bp, but made the overlap 140. Tested on the e. coli strains bl21 and k12. The
        result is a massive SAM file (188 MB !). Visually, this file is hard to go through but borders of what seem
        to be rearrangements are clear. The positional change is only 10bp, so it is very clear where subsequent reads
        do not map as well as the previous ones. It is also very clear where reads begin to map again. Small
        differences in the genome that are (likely) not rearrangement breakpoints are clearly identified as well
        since the reads quickly begin mapping properly again.

    - Pay attention to the bitwise flags (2nd column) in the SAM file as well. With this new alignement, I noticed
        some valuable information in this field that was not apparent in my small tests.

    - Need to find a "sweet spot" for read length and overlap. Consistent read positions might not matter as much
        now, so perhaps we can use variable length. We will see how further tests go.