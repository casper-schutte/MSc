
01/11/21:
    - Have 2 weeks to finish all the coding so that I have 6 weeks to write up.

    - To do:
        1) Fix the code, it cannot handle the positions resetting for different chromosomes.

        2) Test the algorithm/tool on real data

        Sounds easy, but I have not been able to do either.

    - I can't start testing on real data because the algorithm might change. So the priority is fixing the code.
        The problem is that when the consecutive blocks are calculated, they are not associated with the read
        name or number. In the previous iteration of the script, position (relative to the reference) was unique.
        Now, with the introduction of multiple chromosomes, the positions are no longer unique. Thus, I need to change
        the way the consecutive blocks are located or find a way to keep additional data such that the blocks are still
        identifiable.

    - One way I can overcome the problem is by using a different algorithm to return nonconsecutive blocks. I like the
        algorithm. The other way is to try to keep track of the read name while using the same algorithm. I will try
        the latter first.

02/11/21:
    - Made a lot of progress today. The script "consecutive_block_test.py" is working well using the new method.
        There are one or two kinks that might have to be worked out, but I will integrate it into my code and test
        different rearrangements. Then I will test using real data.
    - One thing I need to do is figure out how to extract the "AS" field as this will likely help me to assign borders
        more than just using the mapping quality (MAPQ). Since for FastA files there are reads that were clipped
        but still had a perfect mapping score (uniqueness).

03/11/21:
    - Will conduct some tests today to verify how the adapted algorithm works with different kinds of rearrangements.
        The results will be written up in "experiment_5.txt."
    - First round of tests were successful. Will continue tomorrow.

04/11/21:
    - Will continue with the tests of the new script. As soon as that is done I can start testing real data.

05/11/21:
    - Continued with testing the script on artificial rearrangements. See "experiment_5.txt"

06/11/21:
    - Should be finishing experiments on artificial data. Might make changes on what and how things are returned.

07/11/21:
    - Needed to make small changes. Results very good so far (see experiment_5.txt)
    - Testing multichromosoma data, see experiment_6.txt

08/11/21:
    - Finished experiment 6, see experiment_6.txt for results.
    - Ecerything is working as it should.

09/11/21:
    - Need to incorporate the new algorithm with the full script, so that we can also look at reads that did
        not map properly. These reads should correlate with the borders of rearrangements.

10/11/21:
    - Trying to fix the way that the algorithm deals with duplications. One of the duplicated pair of reads "maps" to
        the right place but the other either does not or does map but in both cases the MAPQ is 1. I would like to
        combine these reads with terrible quality scores into a single block (if they are continuous). This would
        make the output much more clear.
    - Decided it would be better if the borders and blocks were written to a .txt file instead of only being printed
        in the terminal.

13/11/21:
    - Will be trying to change the way reads are generated
    - Changing the way Bowtie2 returns reads such that coverage is increased. Multiple reads will be constructed from
        the same start position but of different lengths.
    - Did not find a way to do this in Bowtie2, but Samtools can do it:

    $ Samtools view -q (MAPQ below which reads are not returned) file_name.sam  > file_name.filtered.sam

    - After filtering out the reads with very low MAPQ (1 or 0, which were the ones that did not map or were
        duplicated), the real data proved too "noisy." My algorithm was too sensitive. Mapping single, slightly
        overlapping reads to construct continuous blocks worked very well for the artificial data. However, the real
        data proved much too noisy. Small differences in the genomes (and in the way reads happen to be mapped -
        remember that Bowtie2 uses random seeds to start looking for alignments).

    - I need to change the way reads are constructed. Multiple reads (of varying lengths) will be constructed from the
        same starting position. This way, the reads will all map where the genomes are the same. Where there is a
        breakpoint, the shorter reads should map but the longer ones map less and less well. This way, small
        differences will not be falsely returned as breakpoints. Instead, the longer (and further along) reads will
        start mapping again, confirming the small change or single badly mapped read is not a breakpoint.

    - We can make the reads overlap more, keeping the current method of construction and return, but I think that
        a more accurate method would use reads of a variable length.

    - Kept the read length at 150 bp, but made the overlap 140. Tested on the e. coli strains bl21 and k12. The
        result is a massive SAM file (188 MB !). Visually, this file is hard to go through but borders of what seem
        to be rearrangements are clear. The positional change is only 10bp, so it is very clear where subsequent reads
        do not map as well as the previous ones. It is also very clear where reads begin to map again. Small
        differences in the genome that are (likely) not rearrangement breakpoints are clearly identified as well
        since the reads quickly begin mapping properly again.

    - Pay attention to the bitwise flags (2nd column) in the SAM file as well. With this new alignment, I noticed
        some valuable information in this field that was not apparent in my small tests.

    - Need to find a "sweet spot" for read length and overlap. Consistent read positions might not matter as much
        now, so perhaps we can use variable length. We will see how further tests go.


14/11/21:
    - Will be testing the heavily overlapping reads on artificial data. I kept the files I tested the older algorithm
        on, but I would like to test on genomes that have multiple rearrangements and some other small differences,
        such that I do not fall into the same over-sensitivity.

    - After visually confirming that this method will work, I need to decide how my script will decide where borders
        are. I can look at consecutive reads and keep track of how they map. When the end of a read crosses a
        rearrangement, it will (should) have a lower MAPQ score than the previous read. It will also show some
        soft-clipping in the CIGAR string (e.g. 135M15S - 135 matching base pairs and 15 soft-clipped ones). These
        properties together should allow the script to pinpoint rearrangement breakpoints.

    - Spent most of the day driving back to stellenbosch.

15/11/21:
    - Will test the algorithm on artificial data again today. Will make multiple rearrangements and use the new method
        for creating overlapping reads. See "Experiment_7.txt" for more details.

17/11/21:
    - Needed to change the algorithm to return reads around (preceding and following) breakpoints (areas with low
        mapq that need to be investigated).

    - Have really been struggling with the code these past few days. What I want the program to do:
        Iterate over every read, start "counting" when the program encounters a read that does not map 100% correctly.
        Then, the program will keep track of how these reads map (position, MAPQ score, CIGAR string).

18/11/21:
    - Had a meeting with Prof who had a few helpful suggestions. We need to greatly increase the coverage. The reads
        will now be constructed from random starting positions until the total length of all the reads is equal to
        the coverage multiplied by the length of the original genome. This will increase the amount of times a
        nucleotide occurs in the reads. We will then look for places where mismatches start appearing from one side
        and assign a border to the last mapping read. This border will be backed up by other reads that contain it.

    - Have successfully changed the script "find_borders_test.py" such that it returns just the part of the cigar
        string that contains the number of matches

    - Have used the result above to write a function that returns the original positions of the breakpoints themselves.
        The result of the aforementioned deletion and mapping are shown below:

chromosome 	 position 	 read name 	 MAPQ 	 CIGAR string
Chr2 	 	 9171 	 	 r2172 	 	 44 	 93M7S
Chr2 	 	 9171 	 	 r2355 	 	 42 	 81M19S
Chr2 	 	 9171 	 	 r2726 	 	 41 	 75M25S
Chr2 	 	 10444 	 	 r1477 	 	 25 	 32S68M
Chr2 	 	 10448 	 	 r1556 	 	 18 	 36S64M
Chr2 	 	 10430 	 	 r1580 	 	 42 	 18S82M
Chr2 	 	 10447 	 	 r2457 	 	 21 	 35S65M
Chr4 	 	 5602 	 	 r4268 	 	 44 	 92M8S
Chr4 	 	 5602 	 	 r3933 	 	 41 	 74M26S
Chr4 	 	 5602 	 	 r4057 	 	 25 	 72M28S
Chr4 	 	 5602 	 	 r4178 	 	 25 	 71M29S
Chr4 	 	 5602 	 	 r3650 	 	 18 	 64M36S
Chr4 	 	 5602 	 	 r4226 	 	 12 	 60M40S
Chr4 	 	 6126 	 	 r3749 	 	 2 	    48S52M
Chr4 	 	 6106 	 	 r3916 	 	 25 	 28S72M
Chr4 	 	 6101 	 	 r4323 	 	 41 	 23S77M
Chr4 	 	 6125 	 	 r4445 	 	 2 	    47S53M
Chr4 	 	 6088 	 	 r4541 	 	 44 	 10S90M
Chr4 	 	 6110 	 	 r4678 	 	 21 	 32S68M

    - Can clearly see that multiple reads confirm the positions of breakpoints. Where there is disagreement, I can
        choose to trust the breakpoint that came from the read with the highest MAPQ or perhaps take the mode.
        This is to be determined.